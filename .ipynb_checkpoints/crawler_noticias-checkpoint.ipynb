{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler de notícias da ufsj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importações e Variaveis Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import scrapy\n",
    "import pandas as pd\n",
    "import scrapy.crawler as crawler\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from multiprocessing import Process, Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza a multipla execução de spiders pelo scrapy\n",
    "# https://stackoverflow.com/questions/41495052/scrapy-reactor-not-restartable\n",
    "def run_spider(spider):\n",
    "    def f(q):\n",
    "        try:\n",
    "            process = CrawlerProcess(get_project_settings())\n",
    "            process.crawl(spider)\n",
    "            process.start()\n",
    "            q.put(None)\n",
    "        except Exception as e:\n",
    "            q.put(e)\n",
    "\n",
    "    q = Queue()\n",
    "    p = Process(target=f, args=(q,))\n",
    "    p.start()\n",
    "    p.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawler usando o Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturando as Metainformações da Noticia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spider para Data, Título e URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoticiasSpider(scrapy.Spider):\n",
    "    name = 'noticias'\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = []\n",
    "        \n",
    "        metadata_ufsj = open('metadata_noticias.json','w')\n",
    "        metadata_ufsj.write('[')\n",
    "        metadata_ufsj.close()\n",
    "        \n",
    "        for i in range (0,14):\n",
    "            urls.append('https://www.ufsj.edu.br/mais_noticias.php?pagina={}'.format(i+1))\n",
    "            \n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse, encoding='utf-8')\n",
    "\n",
    "    def parse(self, response):\n",
    "        #Realizando a escrita dos resultados em arquivo\n",
    "        metadata_ufsj = open('metadata_noticias.json','a')\n",
    "        metadata_ufsj.write(json.dumps({\n",
    "            \"noticia_data\" : response.css(\"#home-subbox-bottom-noticias ul li span::text\").getall(),\n",
    "            \"noticia_titulo\" : response.css(\"#home-subbox-bottom-noticias ul li a::text\").getall(),\n",
    "            \"noticia_url\" : response.css(\"#home-subbox-bottom-noticias ul li a::attr(href)\").getall(),\n",
    "        }))\n",
    "        metadata_ufsj.write(', ')\n",
    "        metadata_ufsj.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chamado de execução para o crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "run_spider(NoticiasSpider)\n",
    "metadata_ufsj = open('metadata_noticias.json','a')\n",
    "metadata_ufsj.write('{}]')\n",
    "metadata_ufsj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geração do Dataframe para o Spider NoticiasSpider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando a leitura dos metadados das notícias\n",
    "df = pd.read_json('metadata_noticias.json')[0:-1]\n",
    "\n",
    "# Planificando o dataframe\n",
    "total = []\n",
    "for k, v in enumerate(df.values):\n",
    "    df_aux = pd.DataFrame({'noticia_data':v[0], 'noticia_titulo':v[1], 'noticia_url':v[2]})\n",
    "    total.append(df_aux)\n",
    "df = pd.concat(total, ignore_index=True)\n",
    "\n",
    "# Ajustando os dados gerados\n",
    "df[\"noticia_data\"] = df[\"noticia_data\"].str[0:-1]\n",
    "df['noticia_url'] = df['noticia_url'].apply(lambda x: 'https://www.ufsj.edu.br/' + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturando o Corpo da Notícia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spider para o corpo da noticia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoticiasBodySpider(scrapy.Spider):\n",
    "    name = \"noticias_body\"\n",
    "    \n",
    "    def start_requests(self):\n",
    "        urls = df['noticia_url']\n",
    "            \n",
    "        print(len(urls))\n",
    "            \n",
    "        data_ufsj = open('data_noticias.json','w')\n",
    "        data_ufsj.write('[')\n",
    "        data_ufsj.close()\n",
    "        \n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse, encoding='utf-8')\n",
    "\n",
    "    def parse(self, response):\n",
    "        noticia_titulo = response.css(\"#home-subbox-bottom-noticias\").xpath(\"./span\").get()\n",
    "        noticia_corpo  = response.css(\"#home-subbox-bottom-noticias\").xpath(\"./div\").get()\n",
    "        rem_html_tags = '<[^>]*>\\s*Voltar\\s*<[^>]*>|<[^>]*>'\n",
    "        \n",
    "        data_ufsj = open('data_noticias.json','a')\n",
    "        \n",
    "        json_resultante = json.dumps({\n",
    "            \"noticia_url\" : response.request.url,\n",
    "            \"noticia_titulo\" : re.sub(rem_html_tags, '', noticia_titulo).strip(),\n",
    "            \"noticia_corpo\" : re.sub(rem_html_tags, '', noticia_corpo).strip(),\n",
    "        }, indent=4)\n",
    "        \n",
    "        data_ufsj.write(json_resultante)\n",
    "        data_ufsj.write(', ')\n",
    "        data_ufsj.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chamado de execução para o crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "run_spider(NoticiasBodySpider)\n",
    "data_ufsj = open('data_noticias.json','a')\n",
    "data_ufsj.write('{}]')\n",
    "data_ufsj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Realizando a leitura dos dados das notícias, como o corpo\n",
    "df_body = pd.read_json('data_noticias.json')[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aglutinando os dois df's gerados (metadados e corpo da notícia)\n",
    "df_completo = pd.merge(df,df_body[['noticia_url', 'noticia_corpo']],on='noticia_url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realizando retoques finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo arquivos auxiliares\n",
    "! rm data_noticias.json metadata_noticias.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando a base final completa em csv, separador é o '\\t'\n",
    "df_completo.to_csv('noticias_ufsj.csv', sep='\\t', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
